{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "document Search & Find in KNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1JZ0USHQq5i1VNG1ZJAdz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitinfaye/Text-classification/blob/main/document_Search_%26_Find_in_KNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3S_TV8DL9Uj"
      },
      "source": [
        "#Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkIYB6Z30elD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUpqBLmWMJQI"
      },
      "source": [
        "import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiYO50xLPPEM"
      },
      "source": [
        "!pip install mglearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GUToUr6LpFR"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "from pandas import Series, DataFrame\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from scipy.sparse import csr_matrix, lil_matrix\n",
        "#from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.sparse import csr_matrix, lil_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsRestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3fhK3UJFyq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1f95568-d181-4947-a733-b640a7091abb"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lzrviELUY7A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "53133c80-45e6-4a6b-bd13-cc884e409630"
      },
      "source": [
        "data_raw = pd.read_csv('/content/abcnews-date-text.csv')\n",
        "data_raw =data_raw[:249]\n",
        "#data_raw = df.drop(['publish_date','tag'], axis=1)\n",
        "#data_raw =df\n",
        "data_raw.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8e86a8e1c3f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/abcnews-date-text.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_raw\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mdata_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m249\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#data_raw = df.drop(['publish_date','tag'], axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#data_raw =df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/abcnews-date-text.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeqyJDoIrUjE"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "lb_make = LabelEncoder()\n",
        "data_raw[\"target\"] = lb_make.fit_transform(data_raw[\"tag\"])\n",
        "data_raw[[\"tag\", \"target\"]].head(11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYky8nB89sRb"
      },
      "source": [
        "data_raw.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFh_h3n2hdgs"
      },
      "source": [
        "'''def convert_publish_date(publish_date):\n",
        "  if publish_date <= 20030219:\n",
        "    out=0\n",
        "  elif publish_date==20030220:\n",
        "    out=1\n",
        "  else:\n",
        "    out=2\n",
        "  return out'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhx83L5rjqXa"
      },
      "source": [
        "'''data_raw['target'] = data_raw['publish_date'].apply(lambda x: convert_publish_date(x))\n",
        "data_raw =data_raw[:249]\n",
        "data_raw.head()'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPEFQbz5L052"
      },
      "source": [
        "data_raw = data_raw.drop('publish_date', axis= 1)\n",
        "print(\"Number of rows in data =\",data_raw.shape[0])\n",
        "print(\"Number of columns in data =\",data_raw.shape[1])\n",
        "print(\"\\n\")\n",
        "print(\"**Sample data:**\")\n",
        "data_raw.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SSrxKe0G3vR"
      },
      "source": [
        "data_raw['tag']= data_raw['tag'].replace(0)\n",
        "data_raw.dropna(inplace=True)\n",
        "missing_values_check = data_raw.isnull().sum()\n",
        "print(missing_values_check)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy3FsSyxJsEF"
      },
      "source": [
        "# Comments with no label are considered to be clean comments.\n",
        "# Creating seperate column in dataframe to identify clean comments.\n",
        "\n",
        "# We use axis=1 to count row-wise and axis=0 to count column wise\n",
        "\n",
        "rowSums = data_raw.iloc[:,2:].sum(axis=1)\n",
        "clean_comments_count = (rowSums==1).sum(axis=0)\n",
        "\n",
        "print(\"Total number of comments = \",len(data_raw))\n",
        "print(\"Number of clean comments = \",clean_comments_count)\n",
        "print(\"Number of comments with labels =\",(len(data_raw)-clean_comments_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51rNiMhsKgV5"
      },
      "source": [
        "categories = list(data_raw.columns.values)\n",
        "categories = categories[2:]\n",
        "print(categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9Zng_9HKuT0"
      },
      "source": [
        "counts = []\n",
        "for category in categories:\n",
        "    counts.append((category, data_raw[category].sum()))\n",
        "df_stats = pd.DataFrame(counts, columns=['category', 'number of comments'])\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMrmT5MK8g8S"
      },
      "source": [
        "sns.set(font_scale = 2)\n",
        "plt.figure(figsize=(15,8))\n",
        "\n",
        "ax= sns.barplot(categories, data_raw.iloc[:,2:].sum().values)\n",
        "\n",
        "plt.title(\"Comments in each category\", fontsize=24)\n",
        "plt.ylabel('Number of comments', fontsize=18)\n",
        "plt.xlabel('Comment Type ', fontsize=18)\n",
        "\n",
        "#adding the text labels\n",
        "rects = ax.patches\n",
        "labels = data_raw.iloc[:,2:].sum().values\n",
        "for rect, label in zip(rects, labels):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ssCiJCl_iSH"
      },
      "source": [
        "rowSums = data_raw.iloc[:,2:].sum(axis=1)\n",
        "multiLabel_counts = rowSums.value_counts()\n",
        "#multiLabel_counts = multiLabel_counts.iloc[1:]\n",
        "\n",
        "sns.set(font_scale = 2)\n",
        "plt.figure(figsize=(15,8))\n",
        "\n",
        "ax = sns.barplot(multiLabel_counts.index, multiLabel_counts.values)\n",
        "\n",
        "plt.title(\"Comments having multiple labels \")\n",
        "plt.ylabel('Number of comments', fontsize=18)\n",
        "plt.xlabel('Number of labels', fontsize=18)\n",
        "\n",
        "#adding the text labels\n",
        "rects = ax.patches\n",
        "labels = multiLabel_counts.values\n",
        "for rect, label in zip(rects, labels):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAJD1nJgMHzZ"
      },
      "source": [
        "WordCloud representation of most used words in each category of comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLSa4PomNr0s"
      },
      "source": [
        "from wordcloud import WordCloud,STOPWORDS\n",
        "\n",
        "plt.figure(figsize=(40,25))\n",
        "\n",
        "# Headline_text\n",
        "#subset = data_raw[data_raw.target==0]\n",
        "text = data_raw.headline_text.values\n",
        "cloud_headline = WordCloud(\n",
        "                          stopwords=STOPWORDS,\n",
        "                          background_color='black',\n",
        "                          collocations=False,\n",
        "                          width=2500,\n",
        "                          height=1800\n",
        "                         ).generate(\" \".join(text))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.axis('off')\n",
        "plt.title(\"Headline_text\",fontsize=40)\n",
        "plt.imshow(cloud_headline)\n",
        "\n",
        "# tag\n",
        "#subset = data_raw[data_raw.tag==0]\n",
        "text = data_raw.tag.values\n",
        "cloud_tag = WordCloud(\n",
        "                          stopwords=STOPWORDS,\n",
        "                          background_color='black',\n",
        "                          collocations=False,\n",
        "                          width=2500,\n",
        "                          height=1800\n",
        "                         ).generate(\" \".join(text))\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.axis('off')\n",
        "plt.title(\"tag\",fontsize=40)\n",
        "plt.imshow(cloud_tag)\n",
        "\n",
        "\n",
        "# Target\n",
        "subset = data_raw[data_raw.target==2]\n",
        "text = data_raw.headline_text.values\n",
        "cloud_headline = WordCloud(\n",
        "                          stopwords=STOPWORDS,\n",
        "                          background_color='black',\n",
        "                          collocations=False,\n",
        "                          width=2500,\n",
        "                          height=1800\n",
        "                         ).generate(\" \".join(text))\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.axis('off')\n",
        "plt.title(\"Target\",fontsize=40)\n",
        "plt.imshow(cloud_headline)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyvkDsZyP8cy"
      },
      "source": [
        "Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U01L9WmdP97X"
      },
      "source": [
        "data = data_raw\n",
        "data = data_raw.loc[np.random.choice(data_raw.index, size=249)]\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rme9VqplQOip"
      },
      "source": [
        "Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JYYngZCQP3Y"
      },
      "source": [
        "def cleanHtml(sentence):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
        "    return cleantext\n",
        "\n",
        "\n",
        "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
        "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
        "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
        "    cleaned = cleaned.strip()\n",
        "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "def keepAlpha(sentence):\n",
        "    alpha_sent = \"\"\n",
        "    for word in sentence.split():\n",
        "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
        "        alpha_sent += alpha_word\n",
        "        alpha_sent += \" \"\n",
        "    alpha_sent = alpha_sent.strip()\n",
        "    return alpha_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNXidkz5QafH"
      },
      "source": [
        "data['headline_text'] = data['headline_text'].str.lower()\n",
        "data['headline_text'] = data['headline_text'].apply(cleanHtml)\n",
        "data['headline_text'] = data['headline_text'].apply(cleanPunc)\n",
        "data['headline_text'] = data['headline_text'].apply(keepAlpha)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3zSdBunRbGm"
      },
      "source": [
        "Removing Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRPz1PUURcZn"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
        "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
        "def removeStopWords(sentence):\n",
        "    global re_stop_words\n",
        "    return re_stop_words.sub(\" \", sentence)\n",
        "\n",
        "data['headline_text'] = data['headline_text'].apply(removeStopWords)\n",
        "#data['tag'] = data['tag'].apply(removeStopWords)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c485z-R8STg6"
      },
      "source": [
        "Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "451d5I9gQK4r"
      },
      "source": [
        "stemmer = SnowballStemmer(\"english\")\n",
        "def stemming(sentence):\n",
        "    stemSentence = \"\"\n",
        "    for word in sentence.split():\n",
        "        stem = stemmer.stem(word)\n",
        "        stemSentence += stem\n",
        "        stemSentence += \" \"\n",
        "    stemSentence = stemSentence.strip()\n",
        "    return stemSentence\n",
        "\n",
        "data['headline_text'] = data['headline_text'].apply(stemming)\n",
        "#data['tag'] = data['tag'].apply(stemming)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReQaD_YxTb1I"
      },
      "source": [
        "#Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6irDuNBVS3dB"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(data, random_state=42, test_size=0.30, shuffle=True)\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoingJoNTD-w"
      },
      "source": [
        "train_text = train['headline_text']\n",
        "test_text = test['headline_text']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3M2MAIqIaq4"
      },
      "source": [
        "train_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVf-dFCdTUVU"
      },
      "source": [
        "#TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoMW1OawTgZy"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
        "vectorizer.fit(train_text)\n",
        "vectorizer.fit(test_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfzhfM0ITp-S"
      },
      "source": [
        "x_train = vectorizer.transform(train_text)\n",
        "y_train = train.drop(labels = ['headline_text','tag'], axis=1)\n",
        "#y_train = train(labels= ['target'],axis=1)\n",
        "\n",
        "x_test = vectorizer.transform(test_text)\n",
        "y_test = test.drop(labels = ['headline_text','tag'], axis=1)\n",
        "#y_test =test(labels= ['target'],axis =1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Deb-Fp8nHbBX"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViW4XNUBQvjX"
      },
      "source": [
        "# Classification "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9noX0y12Q1lu"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Using pipeline for applying logistic regression and one vs rest classifier\n",
        "LogReg_pipeline = Pipeline([\n",
        "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
        "            ])\n",
        "\n",
        "for category in categories:\n",
        "    printmd('**Processing {} comments...**'.format(category))\n",
        "    \n",
        "    # Training logistic regression model on train data\n",
        "    LogReg_pipeline.fit(x_train, train[category])\n",
        "    \n",
        "    # calculating test accuracy\n",
        "    prediction = LogReg_pipeline.predict(x_test)\n",
        "    print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GedNSs7if56"
      },
      "source": [
        "x_train = lil_matrix(x_train).toarray()\n",
        "y_train = lil_matrix(y_train).toarray()\n",
        "x_test = lil_matrix(x_test).toarray()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w55FDWlVHyNI"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExD8XQ3VRDyz"
      },
      "source": [
        "##gaussian naive bayes base allgoritm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKGQ9WC1RDFP"
      },
      "source": [
        "%%time\n",
        "\n",
        "# with a gaussian naive bayes base classifier\n",
        "classifier = GaussianNB()\n",
        "\n",
        "# train\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# predict\n",
        "prediction = classifier.predict(x_test)\n",
        "\n",
        "# accuracy\n",
        "print(\"Accuracy = \",accuracy_score(y_test,prediction))\n",
        "print(\"\\n\")\n",
        "print(\"Training set score: {:.2f}\".format(classifier.score(x_train, y_train)))\n",
        "print(\"Testing set score: {:.2f}\".format(classifier.score(x_test, y_test)))\n",
        "print(\"\\n\")\n",
        "#print(classification_report(y_test,prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "totymck4RVVt"
      },
      "source": [
        "Classifier Chains\n",
        "\n",
        "Multi-label classificattion "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKC4kj3IRNj4"
      },
      "source": [
        "# using classifier chains\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2_K2pPRbRz"
      },
      "source": [
        "%%time\n",
        "\n",
        "# initialize classifier chains multi-label classifier\n",
        "classifier= LogisticRegression()\n",
        "\n",
        "# Training logistic regression model on train data\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# predict\n",
        "prediction = classifier.predict(x_test)\n",
        "\n",
        "# accuracy\n",
        "print(\"Accuracy = \",accuracy_score(y_test,prediction))\n",
        "print(\"\\n\")\n",
        "print(\"Training set score: {:.2f}\".format(classifier.score(x_train, y_train)))\n",
        "print(\"Testing set score: {:.2f}\".format(classifier.score(x_test, y_test)))\n",
        "print(\"\\n\")\n",
        "#print(classification_report(y_test,prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yflbbZeoRtUr"
      },
      "source": [
        "LogisticRegression alogorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPesuBdQR-05"
      },
      "source": [
        "%%time\n",
        "\n",
        "# initialize label powerset multi-label classifier\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# train\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# predict\n",
        "prediction = classifier.predict(x_test)\n",
        "\n",
        "# accuracy\n",
        "print(\"Accuracy = \",accuracy_score(y_test,prediction))\n",
        "print(\"\\n\")\n",
        "print(\"Training set score: {:.2f}\".format(classifier.score(x_train, y_train)))\n",
        "print(\"Testing set score: {:.2f}\".format(classifier.score(x_test, y_test)))\n",
        "print(\"\\n\")\n",
        "#print(classification_report(y_test,prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXvI-C76SF-v"
      },
      "source": [
        "##KNeighborsClassifier Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHIN1zay5ZrC"
      },
      "source": [
        "#knn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzm-pZO4niwU"
      },
      "source": [
        "%%time\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "classifier = KNeighborsClassifier(n_neighbors = 1, metric = 'minkowski', p = 2)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "prediction = classifier.predict(x_test)\n",
        "# accuracy\n",
        "print(\"Accuracy = \",accuracy_score(y_test,prediction))\n",
        "print(\"\\n\")\n",
        "print(\"Training set score: {:.2f}\".format(classifier.score(x_train, y_train)))\n",
        "print(\"Testing set score: {:.2f}\".format(classifier.score(x_test, y_test)))\n",
        "print(\"\\n\")\n",
        "#print(classification_report(y_test,prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baMePYgLoP3b"
      },
      "source": [
        "np.sqrt(250)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPxl7SMNoZdR"
      },
      "source": [
        "%%time\n",
        "classifier = KNeighborsClassifier(n_neighbors =15, metric = 'minkowski', p = 2)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "prediction = classifier.predict(x_test)\n",
        "# accuracy\n",
        "print(\"Accuracy = \",accuracy_score(y_test,prediction))\n",
        "print(\"\\n\")\n",
        "print(\"Training set score: {:.2f}\".format(classifier.score(x_train, y_train)))\n",
        "print(\"Testing set score: {:.2f}\".format(classifier.score(x_test, y_test)))\n",
        "print(\"\\n\")\n",
        "#print(classification_report(y_test,prediction))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFiHKfx02qf9"
      },
      "source": [
        "# KNN Plot\n",
        "#x_train, y_train = mglearn.make_forge()\n",
        "mglearn.plots.plot_knn_classification(n_neighbors=1)\n",
        "mglearn.discrete_scatter(x_train[:, 0], x_train[:, 1], y_train)\n",
        "plt.legend([\"Class 0\", \"Class 1\"], loc=4)\n",
        "plt.xlabel(\"First feature\")\n",
        "plt.ylabel(\"Second feature\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMBdCgNJFci9"
      },
      "source": [
        "#KNN plot\n",
        "#x_train, y_train = mglearn.make_forge()\n",
        "mglearn.plots.plot_knn_classification(n_neighbors=15)\n",
        "mglearn.discrete_scatter(x_train[:, 0], x_train[:, 1], y_train)\n",
        "plt.legend([\"Class 0\", \"Class 1\"], loc=4)\n",
        "plt.xlabel(\"First feature\")\n",
        "plt.ylabel(\"Second feature\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkfXQICWPk_t"
      },
      "source": [
        "#Visualising the Training set results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBVBjejitMK5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP05PhZetMdL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}