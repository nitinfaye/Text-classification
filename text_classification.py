# -*- coding: utf-8 -*-
"""Text_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EctFzhY7omL98a8d7ZDNS-zK5u5REpiH

*Owner*
 
NItin Faye

**business Problem**

Meta data: 
1. Text : contains text from blockchain domain
2. Target : traget class

Prepare a report (pdf/pptx) based on your model result. 
Report should contain below points. 
1. Your thoughts on problem or what is your approach to solve the problem
2. Model Interpretation
3. Train & test accuracy score, classification report
4. Limitation of the model. 
5. You can add your own points as well

Create a classification model based on the below mentioned dataset.

Dataset Link: https://docs.google.com/spreadsheets/d/1DLL6BTXiHHsn1w9NvVi0BZbass0QU0RSvKEXtJlwfCM/edit?usp=sharing
"""

import os
import csv
import pandas as pd
from pandas import Series, DataFrame
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from sklearn.multiclass import OneVsRestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report,confusion_matrix
from scipy.sparse import csr_matrix, lil_matrix
#from skmultilearn.problem_transform import BinaryRelevance
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.metrics import accuracy_score
from scipy.sparse import csr_matrix, lil_matrix
from sklearn.metrics import accuracy_score
from sklearn.multiclass import OneVsRestClassifier

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
import re

import sys
import warnings

if not sys.warnoptions:
    warnings.simplefilter("ignore")
from IPython.display import Markdown, display
def printmd(string):
    display(Markdown(string))

from google.colab import drive
drive.mount('/content/drive/')

data = pd.read_csv("/content/drive/MyDrive/root2ai.csv")
data.head()

from sklearn.preprocessing import LabelEncoder

lb_make = LabelEncoder()
data["Tag"] = lb_make.fit_transform(data["Target"])
data[["Target", "Tag"]].head(11)

data.shape

data['Target']= data['Target'].replace(0)
data.dropna(inplace=True)
missing_values_check = data.isnull().sum()
print(missing_values_check)

rowSums = data.iloc[:,2:].sum(axis=1)
clean_comments_count = (rowSums==1).sum(axis=0)

print("Total number of comments = ",len(data))
print("Number of clean comments = ",clean_comments_count)
print("Number of comments with labels =",(len(data)-clean_comments_count))

categories = list(data.columns.values)
categories = categories[2:]
print(categories)

counts = []
for category in categories:
    counts.append((category, data[category].sum()))
df_stats = pd.DataFrame(counts, columns=['Category', 'Number of Comments'])
df_stats

sns.set(font_scale = 2)
plt.figure(figsize=(15,8))

ax= sns.barplot(categories, data.iloc[:,2:].sum().values)

plt.title("Comments in each category", fontsize=16)
plt.ylabel('Number of comments', fontsize=10)
plt.xlabel('Comment Type ', fontsize=10)

#adding the text labels
rects = ax.patches
labels = data.iloc[:,2:].sum().values
for rect, label in zip(rects, labels):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=10)

plt.show()



rowSums = data.iloc[:,2:].sum(axis=1)
multiLabel_counts = rowSums.value_counts()
#multiLabel_counts = multiLabel_counts.iloc[1:]

sns.set(font_scale = 2)
plt.figure(figsize=(15,8))

ax = sns.barplot(multiLabel_counts.index, multiLabel_counts.values)

plt.title("Comments having multiple labels ")
plt.ylabel('Number of comments', fontsize=10)
plt.xlabel('Number of labels', fontsize=10)

#adding the text labels
rects = ax.patches
labels = multiLabel_counts.values
for rect, label in zip(rects, labels):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')

plt.show()

"""#WordCloud representation of most used words in each category of comments"""

from wordcloud import WordCloud,STOPWORDS

plt.figure(figsize=(40,25))

# Text
subset = data[data.Text==0]
text = data.Text.values
cloud_Text = WordCloud(
                          stopwords=STOPWORDS,
                          background_color='black',
                          collocations=False,
                          width=2500,
                          height=1800
                         ).generate(" ".join(text))

plt.subplot(2, 3, 1)
plt.axis('off')
plt.title("Text",fontsize=40)
plt.imshow(cloud_Text)

# Target
subset = data[data.Target==1]
text = data.Target.values
cloud_Target = WordCloud(
                          stopwords=STOPWORDS,
                          background_color='black',
                          collocations=False,
                          width=2500,
                          height=1800
                         ).generate(" ".join(text))

plt.subplot(2, 3, 2)
plt.axis('off')
plt.title("Target",fontsize=40)
plt.imshow(cloud_Target)

"""##Data Pre-Processing"""

data = data.loc[np.random.choice(data.index, size=5000)]
data.shape

"""Cleaning Data"""

def cleanHtml(sentence):
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, ' ', str(sentence))
    return cleantext


def cleanPunc(sentence): #function to clean the word of any punctuation or special characters
    cleaned = re.sub(r'[?|!|\'|"|#]',r'',sentence)
    cleaned = re.sub(r'[.|,|)|(|\|/]',r' ',cleaned)
    cleaned = cleaned.strip()
    cleaned = cleaned.replace("\n"," ")
    return cleaned


def keepAlpha(sentence):
    alpha_sent = ""
    for word in sentence.split():
        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)
        alpha_sent += alpha_word
        alpha_sent += " "
    alpha_sent = alpha_sent.strip()
    return alpha_sent

data['Text'] = data['Text'].str.lower()
data['Text'] = data['Text'].apply(cleanHtml)
data['Text'] = data['Text'].apply(cleanPunc)
data['Text'] = data['Text'].apply(keepAlpha)

"""Removing Stop Words"""

stop_words = set(stopwords.words('english'))
stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])
re_stop_words = re.compile(r"\b(" + "|".join(stop_words) + ")\\W", re.I)
def removeStopWords(sentence):
    global re_stop_words
    return re_stop_words.sub(" ", sentence)

data['Text'] = data['Text'].apply(removeStopWords)
#data['tag'] = data['tag'].apply(removeStopWords)

"""Stemming"""

stemmer = SnowballStemmer("english")
def stemming(sentence):
    stemSentence = ""
    for word in sentence.split():
        stem = stemmer.stem(word)
        stemSentence += stem
        stemSentence += " "
    stemSentence = stemSentence.strip()
    return stemSentence

data['Text'] = data['Text'].apply(stemming)
#data['tag'] = data['tag'].apply(stemming)

"""#Train-Test Split

"""

from sklearn.model_selection import train_test_split

train, test = train_test_split(data, random_state=42, test_size=0.30, shuffle=True)

print(train.shape)
print(test.shape)

train_text = train['Text']
test_text = test['Text']
print(train_text)

"""#TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')
vectorizer.fit(train_text)
vectorizer.fit(test_text)

x_train = vectorizer.transform(train_text)
y_train = train.drop(labels = ['Text','Target'], axis=1)
#y_train = train(labels= ['target'],axis=1)

x_test = vectorizer.transform(test_text)
y_test = test.drop(labels = ['Text','Target'], axis=1)

x_train

"""#Classification"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Using pipeline for applying logistic regression and one vs rest classifier
# LogReg_pipeline = Pipeline([
#                 ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),
#             ])
# 
# for category in categories:
#     printmd('**Processing {} comments...**'.format(category))
#     
#     # Training logistic regression model on train data
#     LogReg_pipeline.fit(x_train, train[category])
#     
#     # calculating test accuracy
#     prediction = LogReg_pipeline.predict(x_test)
#     print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))
#     print("\n")

x_train = lil_matrix(x_train).toarray()
y_train = lil_matrix(y_train).toarray()
x_test = lil_matrix(x_test).toarray()

x_train

"""##Gaussian Naive bayes base allgoritm"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # with a gaussian naive bayes base classifier
# classifier = GaussianNB()
# 
# # train
# classifier.fit(x_train, y_train)
# 
# # predict
# prediction = classifier.predict(x_test)
# 
# # accuracy
# print("Accuracy = ",accuracy_score(y_test,prediction))
# print("\n")
# print("Training set score: {:.2f}".format(classifier.score(x_train, y_train)))
# print("Testing set score: {:.2f}".format(classifier.score(x_test, y_test)))
# print("\n")
# print(classification_report(y_test,prediction))

"""Classifier Chains

Multi-label classificattion 
"""

# using classifier chains
from sklearn.linear_model import LogisticRegression

"""LogisticRegression alogorithms"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # initialize classifier chains multi-label classifier
# classifier= LogisticRegression()
# 
# # Training logistic regression model on train data
# classifier.fit(x_train, y_train)
# 
# # predict
# prediction = classifier.predict(x_test)
# 
# # accuracy
# print("Accuracy = ",accuracy_score(y_test,prediction))
# print("\n")
# print("Training set score: {:.2f}".format(classifier.score(x_train, y_train)))
# print("Testing set score: {:.2f}".format(classifier.score(x_test, y_test)))
# print("\n")
# print(classification_report(y_test,prediction))

"""##KNeighborsClassifier Algorithm"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.neighbors import KNeighborsClassifier
# 
# classifier = KNeighborsClassifier(n_neighbors = 1, metric = 'minkowski', p = 2)
# classifier.fit(x_train, y_train)
# 
# prediction = classifier.predict(x_test)
# print("\n")
# # accuracy
# print("Accuracy = ",accuracy_score(y_test,prediction))
# print("\n")
# print("Training set score: {:.2f}".format(classifier.score(x_train, y_train)))
# print("Testing set score: {:.2f}".format(classifier.score(x_test, y_test)))
# print("\n")
# print(classification_report(y_test,prediction))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# classifier = KNeighborsClassifier(n_neighbors =7, metric = 'minkowski', p = 2)
# classifier.fit(x_train, y_train)
# 
# prediction = classifier.predict(x_test)
# # accuracy
# print("Accuracy = ",accuracy_score(y_test,prediction))
# print("\n")
# print("Training set score: {:.2f}".format(classifier.score(x_train, y_train)))
# print("Testing set score: {:.2f}".format(classifier.score(x_test, y_test)))
# print("\n")
# print(classification_report(y_test,prediction))
# 
#

"""##ARTIFITIAL NEURAL NETWORK
Building the ANN
"""

import tensorflow as tf
tf.__version__

ann = tf.keras.models.Sequential()

"""### Adding the input layer and the first hidden layer"""

ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

"""### Adding the second hidden layer"""

ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

"""### Adding the output layer"""

ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

"""##Training the ANN

Compiling the ANN
"""

ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

"""### Training the ANN on the Training set"""

ann.fit(x_train, y_train, batch_size = 32, epochs = 100)

"""### Predicting the Test set results"""

y_pred = ann.predict(x_test)

"""### Making the Confusion Matrix"""

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
accuracy_score(y_test, y_pred)